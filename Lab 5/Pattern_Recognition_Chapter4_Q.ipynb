{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gqGnXjC96dQF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_tJ_Alt22YgP"
      },
      "outputs": [],
      "source": [
        "def add_intercept(x):\n",
        "    \"\"\"\n",
        "      Dùng cho tất cả các bài trong file\n",
        "\n",
        "    Args:\n",
        "        x: 2D NumPy array.\n",
        "\n",
        "    Returns:\n",
        "        New matrix same as x with 1's in the 0th column.\n",
        "    \"\"\"\n",
        "    new_x = np.zeros((x.shape[0], x.shape[1] + 1), dtype=x.dtype)\n",
        "    new_x[:, 0] = 1\n",
        "    new_x[:, 1:] = x\n",
        "\n",
        "    return new_x\n",
        "\n",
        "\n",
        "def load_dataset(csv_path, label_col='y', bias=False):\n",
        "    \"\"\"\n",
        "    Dùng cho dataset ds1_train\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Validate label_col argument\n",
        "    allowed_label_cols = ('y', 't')\n",
        "    if label_col not in allowed_label_cols:\n",
        "        raise ValueError('Invalid label_col: {} (expected {})'\n",
        "                         .format(label_col, allowed_label_cols))\n",
        "\n",
        "    # Load headers\n",
        "    with open(csv_path, 'r') as csv_fh:\n",
        "        headers = csv_fh.readline().strip().split(',')\n",
        "\n",
        "    # Load features and labels\n",
        "    x_cols = [i for i in range(len(headers)) if headers[i].startswith('x')]\n",
        "    l_cols = [i for i in range(len(headers)) if headers[i] == label_col]\n",
        "    inputs = np.loadtxt(csv_path, delimiter=',', skiprows=1, usecols=x_cols)\n",
        "    labels = np.loadtxt(csv_path, delimiter=',', skiprows=1, usecols=l_cols)\n",
        "\n",
        "    if inputs.ndim == 1:\n",
        "        inputs = np.expand_dims(inputs, -1)\n",
        "\n",
        "    if bias:\n",
        "        inputs = add_intercept(inputs)\n",
        "\n",
        "    return inputs, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5zzNScaesIz"
      },
      "source": [
        "#I. Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLNoW7R5LM26"
      },
      "source": [
        "Trong các bài học trước, ta đã được học về mô hình **Hồi quy Tuyến tính (Linear Regression)**, mô hình này hoạt động hiệu quả khi **biến đầu ra (target)** là **liên tục** — chẳng hạn như dự đoán giá nhà, chiều cao, cân nặng,... Tuy nhiên, khi ta cần **phân loại** (ví dụ: email có phải spam hay không, bệnh nhân mắc bệnh hay không), thì **Linear Regression không còn phù hợp**.\n",
        "\n",
        "Vì vậy, ta cần một mô hình khác — **Logistic Regression**, được thiết kế cho **phân loại nhị phân**. Logistic Regression vẫn giữ phần **kết hợp tuyến tính** của đặc trưng đầu vào, nhưng sử dụng **hàm logistic (sigmoid)** để **nén đầu ra** về khoảng $[0,1]$, giúp diễn giải trực tiếp như **xác suất**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvbZbQwpvYe4"
      },
      "source": [
        "Gợi nhắc lại mô hình hồi quy tuyến tính, ta có:\n",
        "$$\n",
        "h_{\\theta}(x)= \\theta_0 + \\theta_1 x_1 + \\ldots \\theta_n x_n\n",
        "$$\n",
        "\n",
        "hay\n",
        "\n",
        "$$\n",
        "h(x)= \\sum_{i=0}^{n} \\theta_i x_i = \\theta^{T}x\n",
        "$$\n",
        "\n",
        "Trong bài toán phân loại, ta không thể sử dụng trực tiếp Linear Regression để dự đoán nhãn ví dụ như $y \\in \\{0,1\\}$. Lý do là vì đầu ra của mô hình tuyến tính có thể lớn hơn 1 hoặc nhỏ hơn 0, trong khi ta cần một giá trị biểu diễn **xác suất.**\n",
        "\n",
        "Để khắc phục vấn đề này, ta thay đổi dạng của giả thuyết $h_{\\theta}(x)$ sao cho giá trị đầu ra luôn nằm trong khoảng $[0,1]$ bằng cách:\n",
        "\n",
        "$$\n",
        "h_{\\theta}(x) = g(\\theta^Tx)\n",
        "$$\n",
        "\n",
        "với $g(z)$ là hàm logistic (sigmoid function) được định nghĩa như sau:\n",
        "$$\n",
        "g(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ecvH57kVetSZ"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ZPol4YyC7nuF",
        "outputId": "d9674d81-e354-415e-d57d-f8c9698cf7fb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "x = np.linspace(-10, 10, 100)\n",
        "y = sigmoid(x)\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(x, y, color='blue', linewidth=2, label='sigmoid(x)')\n",
        "plt.title(\"Sigmoid Function\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"g(x)\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAEco0boYxE0"
      },
      "source": [
        "Từ hình vẽ bên trên, ta có thể thấy khi $g(z)$ tiến tới $1$ khi $z \\to \\infty$, và $g(z)$ tiến tới $0$ khi $z \\to -\\infty$. Do đó, $g(z)$, và cả $h(\\mathbf{x})$, luôn bị chặn giữa $0$ và $1$. Như với mô hình Linear Regression trong bài trước, chúng ta vẫn giữ quy ước đặt $x_0 = 1$, sao cho\n",
        "$$\\boldsymbol{\\theta}^T \\mathbf{x} = \\theta_0 + \\sum_{j=1}^n \\theta_j x_j.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE5d2AU17phM"
      },
      "source": [
        "Nhờ đặc tính của hàm sigmoid, đầu ra của Hồi quy Logistic (Logistic Regression) có thể được diễn giải như là **xác suất** mà mẫu $\\mathbf{x}$ thuộc về lớp $1$:\n",
        "\n",
        "$$P(y=1|\\mathbf{x};\\boldsymbol{\\theta}) = h_{\\boldsymbol{\\theta}}(\\mathbf{x}) = g(\\boldsymbol{\\theta}^T \\mathbf{x})$$\n",
        "\n",
        "Ngược lại, xác suất mẫu $\\mathbf{x}$ thuộc về lớp $0$ sẽ là:\n",
        "\n",
        "$$P(y=0|\\mathbf{x};\\boldsymbol{\\theta}) = 1 - h_{\\boldsymbol{\\theta}}(\\mathbf{x})$$\n",
        "\n",
        "Dựa trên hai xác suất trên, chúng ta có thể viết gọn công thức xác suất cho cả hai trường hợp $y \\in \\{0, 1\\}$ thành một biểu thức duy nhất:\n",
        "\n",
        "$$P(y|\\mathbf{x};\\boldsymbol{\\theta}) = h_{\\boldsymbol{\\theta}}(\\mathbf{x})^{y}(1-h_{\\boldsymbol{\\theta}}(\\mathbf{x}))^{1-y}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_jWqETg5F77"
      },
      "source": [
        "Giả sử có $m$ ví dụ huấn luyện $(\\mathbf{x}^{(i)}, y^{(i)})$ được tạo ra **độc lập và phân phối đồng nhất (I.I.D)**. **Hàm Hợp Lý** $L(\\boldsymbol{\\theta})$ là tích của xác suất của tất cả các ví dụ:\n",
        "\n",
        "$$L(\\boldsymbol{\\theta}) = P(\\mathbf{y}|\\mathbf{X}; \\boldsymbol{\\theta}) = \\prod_{i=1}^{m} P(y^{(i)}|\\mathbf{x}^{(i)}; \\boldsymbol{\\theta})$$\n",
        "\n",
        "Thay công thức xác suất gộp vào:\n",
        "\n",
        "$$L(\\boldsymbol{\\theta}) = \\prod_{i=1}^{m} \\left( h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) \\right)^{y^{(i)}} \\left( 1 - h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) \\right)^{1-y^{(i)}}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_EWpRsQ5OsS"
      },
      "source": [
        "Để việc tối ưu hóa và lấy đạo hàm thuận tiện hơn, chúng ta chuyển sang **Hàm Log-Hợp Lý** $\\ell(\\boldsymbol{\\theta}) = \\log L(\\boldsymbol{\\theta})$:\n",
        "\n",
        "$$\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{m} \\left[ y^{(i)} \\log \\left( h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) \\right) + (1 - y^{(i)}) \\log \\left( 1 - h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) \\right) \\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSrqQJFn5T2N"
      },
      "source": [
        "Chúng ta định nghĩa **Hàm Chi Phí** $J(\\boldsymbol{\\theta})$ (còn gọi là **Cross-Entropy Loss** hoặc **Log Loss**) là **âm** của Log-Hợp Lý, chia cho $m$ để lấy chi phí trung bình:\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = - \\frac{1}{m} \\ell(\\boldsymbol{\\theta})$$\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log \\left( h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) \\right) + (1 - y^{(i)}) \\log \\left( 1 - h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) \\right) \\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nug438sJ5ehu"
      },
      "source": [
        "#II. Gradient Descent cho Logistic Regression\n",
        "\n",
        "Trong Hồi quy Tuyến tính, Gradient Descent có dạng (sử dụng hàm mất mát bình phương trung bình - Mean Squared Error):\n",
        "\n",
        "**Quy tắc cập nhật tham số $\\boldsymbol{\\theta}$:**\n",
        "$$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) - y^{(i)} \\right) x_j^{(i)}$$\n",
        "\n",
        "***\n",
        "\n",
        "Trong **Hồi quy Logistic**, Gradient Descent được sử dụng để **tối thiểu hóa (Minimize) Hàm Chi Phí Cross-Entropy (Log Loss)** $J(\\boldsymbol{\\theta})$.\n",
        "\n",
        "### 1. Hàm Chi Phí (Cost Function)\n",
        "$$J(\\boldsymbol{\\theta}) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log \\left( h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) \\right) + (1 - y^{(i)}) \\log \\left( 1 - h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) \\right) \\right]$$\n",
        "\n",
        "Trong đó $h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) = g(\\boldsymbol{\\theta}^T \\mathbf{x}^{(i)}) = \\frac{1}{1 + e^{-\\boldsymbol{\\theta}^T \\mathbf{x}^{(i)}}}$.\n",
        "\n",
        "### 2. Đạo hàm (Gradient)\n",
        "Đạo hàm riêng của hàm chi phí $J(\\boldsymbol{\\theta})$ theo tham số $\\theta_j$ là:\n",
        "$$\\frac{\\partial}{\\partial \\theta_j} J(\\boldsymbol{\\theta}) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) - y^{(i)} \\right) x_j^{(i)}$$\n",
        "\n",
        "hay:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J(\\theta) = \\frac{1}{m}  X^T (h - y)\n",
        "$$\n",
        "\n",
        "### 3. Quy tắc Cập nhật Gradient Descent\n",
        "Quy tắc cập nhật cho từng tham số $\\theta_j$ (với $\\alpha$ là tốc độ học):\n",
        "\n",
        "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\boldsymbol{\\theta})$$\n",
        "\n",
        "Thay đạo hàm vào, ta được quy tắc cập nhật Gradient Descent cho **Hồi quy Logistic**:\n",
        "\n",
        "$$\\theta_j := \\theta_j - \\alpha \\frac{1}{m}  X^T (h - y)$$\n",
        "\n",
        "**Lưu ý quan trọng:**\n",
        "\n",
        "Mặc dù quy tắc cập nhật của Logistic Regression **trông giống hệt** Linear Regression, chúng là hai thuật toán khác nhau vì hàm giả thuyết $h_{\\boldsymbol{\\theta}}(\\mathbf{x})$ được định nghĩa khác:\n",
        "\n",
        "* **Linear Regression:** $h_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\boldsymbol{\\theta}^T \\mathbf{x}$ (Tuyến tính)\n",
        "* **Logistic Regression:** $h_{\\boldsymbol{\\theta}}(\\mathbf{x}) = g(\\boldsymbol{\\theta}^T \\mathbf{x})$ (Phi tuyến tính, sử dụng hàm Sigmoid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1mR92Geh2bn_"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Lab 5/ds1_train.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m ds1_training_set_path = \u001b[33m'\u001b[39m\u001b[33m/content/Lab 5/ds1_train.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      2\u001b[39m ds1_valid_set_path = \u001b[33m'\u001b[39m\u001b[33m/content/Lab 5/ds1_valid.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m x_train, y_train = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds1_training_set_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m x_valid, y_valid = load_dataset(ds1_valid_set_path, bias=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(csv_path, label_col, bias)\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mInvalid label_col: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m (expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     28\u001b[39m                      .format(label_col, allowed_label_cols))\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Load headers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csv_fh:\n\u001b[32m     32\u001b[39m     headers = csv_fh.readline().strip().split(\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Load features and labels\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\Program Files\\python3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/Lab 5/ds1_train.csv'"
          ]
        }
      ],
      "source": [
        "ds1_training_set_path = '/content/Lab 5/ds1_train.csv'\n",
        "ds1_valid_set_path = '/content/Lab 5/ds1_valid.csv'\n",
        "\n",
        "x_train, y_train = load_dataset(ds1_training_set_path, bias=True)\n",
        "x_valid, y_valid = load_dataset(ds1_valid_set_path, bias=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaVXfWSTMSTY"
      },
      "source": [
        "# **Bài tập 1:** Hoàn thiện class `SimpleLogisticRegression`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKSWfReOHgTm"
      },
      "outputs": [],
      "source": [
        "class SimpleLogisticRegression(object):\n",
        "    def __init__(self, alpha=0.1, eps=1e-6, max_iter=1000, verbose: bool = False):\n",
        "        self.theta = None\n",
        "        self.alpha = alpha\n",
        "        self.eps = eps\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def compute_loss(self, X, y):\n",
        "        m = len(y)\n",
        "        # TODO: 1. Tính h sử dụng hàm sigmoid.\n",
        "        h = self.sigmoid(X @ self.theta)\n",
        "        # TODO: 2. Áp dụng công thức Log Loss\n",
        "        loss = - (1 / m) * np.sum(y * np.log(h + self.eps) + (1 - y) * np.log(1 - h + self.eps))\n",
        "        return loss\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "        m = len(y)\n",
        "        # TODO: 1. Tính xác suất dự đoán h.\n",
        "        h = self.sigmoid(X @ self.theta)\n",
        "        # TODO: 2. Tính sai số.\n",
        "        error = h - y\n",
        "        # TODO: 3. Tính Gradient.\n",
        "        grad = (1 / m) * (X.T @ error)\n",
        "        return grad\n",
        "        # TODO: 4. Cập nhật theta: new_theta = self.theta - self.alpha * grad.\n",
        "        new_theta = self.theta - self.alpha * grad\n",
        "        # TODO: Trả về new_theta.\n",
        "        return new_theta\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n_features = X.shape\n",
        "        # TODO: 1. Khởi tạo self.theta bằng vector zero (kích thước n_features).\n",
        "        self.theta = np.zeros(n_features)\n",
        "        for i in range(self.max_iter):\n",
        "\n",
        "            # TODO: 2. Lấy new_theta bằng cách gọi self._gradient_descent_step().\n",
        "            new_theta = self._gradient_descent(X, y)\n",
        "            # TODO: 3. KIỂM TRA HỘI TỤ (Convergence Check):\n",
        "            # Dùng np.linalg.norm(new_theta - self.theta, 1) < self.eps.\n",
        "            # Nếu hội tụ, cập nhật self.theta và kết thúc (return).\n",
        "            if np.linalg.norm(new_theta - self.theta, 1) < self.eps:\n",
        "                self.theta = new_theta\n",
        "                return\n",
        "            # TODO: 4. Cập nhật self.theta = new_theta.\n",
        "            self.theta = new_theta\n",
        "            # TODO: 5. (Tùy chọn) In thông tin mất mát (loss) nếu self.verbose là True.\n",
        "            if self.verbose and i % 100 == 0:\n",
        "                loss = self.compute_loss(X, y)\n",
        "                print(f\"Iteration {i}: Loss = {loss}\")\n",
        "            pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        # TODO: 1. Tính xác suất dự đoán prob = self.sigmoid(X @ self.theta).\n",
        "        prob = self.sigmoid(X @ self.theta)\n",
        "        # TODO: 2. Áp dụng ngưỡng (threshold) 0.5 để đưa ra nhãn cuối cùng (0 hoặc 1).\n",
        "        # Gợi ý: Dùng phép so sánh và .astype(int).\n",
        "        return (prob >= 0.5).astype(int)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQf-F-LIzS3N"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'x_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Sau khi hoàn thiện class trên thì uncomment các dòng dưới đây,\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Cho phép điều chỉnh lại các hyper-parameters theo ý thích của mình.\u001b[39;00m\n\u001b[32m      4\u001b[39m model = SimpleLogisticRegression(alpha=\u001b[32m0.001\u001b[39m, max_iter=\u001b[32m10000\u001b[39m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model.fit(\u001b[43mx_train\u001b[49m, y_train)\n\u001b[32m      6\u001b[39m learned_theta = model.theta\n",
            "\u001b[31mNameError\u001b[39m: name 'x_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Sau khi hoàn thiện class trên thì uncomment các dòng dưới đây,\n",
        "# Cho phép điều chỉnh lại các hyper-parameters theo ý thích của mình.\n",
        "\n",
        "model = SimpleLogisticRegression(alpha=0.001, max_iter=10000, verbose=True)\n",
        "model.fit(x_train, y_train)\n",
        "learned_theta = model.theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef4ppmR0zjPd",
        "outputId": "864b1716-bda0-4511-b9aa-bfe7dd878537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Learned Parameters (theta):\n",
            "[-0.47080079  0.70797107 -0.02234002]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nLearned Parameters (theta):\")\n",
        "print(learned_theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E0J1-s-zmf5",
        "outputId": "9caaa0be-286a-4237-a10b-bfe49f77877d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Accuracy on Training Data: 0.80\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(x_valid)\n",
        "accuracy = np.mean(y_pred == y_valid)\n",
        "print(f\"\\nFinal Accuracy on Valid Data: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zThHSDBKqkuT"
      },
      "source": [
        "# III. Phương pháp Newton cho Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sL1V361q0qc"
      },
      "source": [
        "Ngoài Gradient Descent, ta còn một phương pháp khác để tối ưu hóa Logistic Regression là sử dụng phương pháp Newton để tìm nghiệm của hàm đạo hàm của hàm mục tiêu.\n",
        "\n",
        "Để bắt đầu, chúng ta hãy xem xét **phương pháp Newton** để tìm nghiệm của một hàm. Cụ thể, giả sử chúng ta có một hàm $f : \\mathbb{R} \\mapsto \\mathbb{R}$, và chúng ta muốn tìm một giá trị $\\theta$ sao cho $f(\\theta) = 0$. Ở đây, $\\theta \\in \\mathbb{R}$ là một số thực. Phương pháp Newton thực hiện cập nhật sau:\n",
        "$$\\theta := \\theta - \\frac{f(\\theta)}{f'(\\theta)}$$\n",
        "\n",
        "Phương pháp này có một cách giải thích tự nhiên, trong đó chúng ta có thể coi nó là việc xấp xỉ hàm $f$ bằng một hàm tuyến tính tiếp tuyến với $f$ tại giá trị đoán hiện tại $\\theta$, giải phương trình để tìm nơi hàm tuyến tính đó bằng không, và để giá trị đoán tiếp theo cho $\\theta$ là nơi hàm tuyến tính đó bằng `0`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE8tyfQQBEtB"
      },
      "source": [
        "Bằng cách đặt $f(\\theta) = \\ell'(\\theta)$, chúng ta có thể sử dụng cùng một thuật toán để tối đa hóa $\\ell$, và ta nhận được quy tắc cập nhật:\n",
        "\n",
        "$$\\theta := \\theta - \\frac{\\ell'(\\theta)}{\\ell''(\\theta)}$$\n",
        "\n",
        "hay:\n",
        "$$\n",
        "\\theta := \\theta - H^{-1}\\nabla_{\\theta}\\ell(\\theta)\n",
        "$$\n",
        "\n",
        "\n",
        "biết:\n",
        "$$\n",
        "H = X^T W X\n",
        "$$\n",
        "- Trong đó:\n",
        "  - W là Ma trận Đường chéo (m x m).\n",
        "  - Các phần tử trên đường chéo là:\n",
        "  - $W_{ii} = h_i (1 - h_i)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZoRVfieLWFS"
      },
      "source": [
        "Dựa trên hai công thức ở trên, hãy xây dựng một funciton hoặc một class để xử lý Logistic Regression sử dụng phương pháp Newton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJvW_JfJMMG-"
      },
      "source": [
        "# **Bài tập 2:** Hoàn thiện class `LogisticNewtonMethod`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BvrniQNzLSz"
      },
      "outputs": [],
      "source": [
        "class LogisticNewtonMethod(object):\n",
        "    def __init__(self, eps=1e-6, max_iter=100, verbose: bool = False):\n",
        "        self.theta = None\n",
        "        self.eps = eps\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def compute_loss(self, X, y):\n",
        "        m = len(y)\n",
        "        # TODO: 1. Tương tự Bt1\n",
        "        h = self.sigmoid(X @ self.theta)\n",
        "        loss = - (1 / m) * np.sum(y * np.log(h + self.eps) + (1 - y) * np.log(1 - h + self.eps))\n",
        "        return loss\n",
        "        pass\n",
        "    def hessian(self, X):\n",
        "        m, n_features = X.shape\n",
        "        # TODO: 1. Tính h.\n",
        "        h = self.sigmoid(X @ self.theta)\n",
        "        # TODO: 2. Tính vector w = h * (1 - h).\n",
        "        w = h * (1 - h)\n",
        "        # TODO: 3. Tạo ma trận đường chéo W từ vector w (np.diag(w)).\n",
        "        W = np.diag(w)\n",
        "        # TODO: 4. Tính toán và trả về ma trận Hessian.\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n_features = X.shape\n",
        "        # TODO: 1. Khởi tạo self.theta bằng vector zero (kích thước n_features).\n",
        "        self.theta = np.zeros(n_features)\n",
        "        for i in range(self.max_iter):\n",
        "\n",
        "            # TODO: 2. Tính Gradient:\n",
        "            grad = self._gradient_descent(X, y)\n",
        "            # TODO: 3. Tính Ma trận Hessian $H$ bằng cách gọi self.hessian(X).\n",
        "            H = self.hessian(X)\n",
        "            # TODO: 4. Tính nghịch đảo của Hessian: np.linalg.pinv.\n",
        "            H_inv = np.linalg.pinv(H)\n",
        "            # TODO: 5. Tính bước thay đổi H^(-1) @ grad.\n",
        "            delta = H_inv @ grad\n",
        "\n",
        "            # TODO: 6. Cập nhật theta.\n",
        "            new_theta = self.theta - delta\n",
        "\n",
        "            # TODO: 7. KIỂM TRA HỘI TỤ (Convergence Check):\n",
        "            # Dùng np.linalg.norm(new_theta - self.theta, 1) < self.eps.\n",
        "            # Nếu hội tụ, cập nhật self.theta và kết thúc (return).\n",
        "            if np.linalg.norm(new_theta - self.theta, 1) < self.eps:\n",
        "                self.theta = new_theta\n",
        "                return\n",
        "\n",
        "            # TODO: 8. Cập nhật self.theta = new_theta (có thể skip nếu đã update self.theta ở B6)\n",
        "            self.theta = new_theta\n",
        "            # TODO: 9. (Tùy chọn) In thông tin mất mát nếu self.verbose là True.\n",
        "            if self.verbose:\n",
        "                loss = self.compute_loss(X, y)\n",
        "                print(f\"Iteration {i + 1}: Loss = {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        # TODO: 1. Tính xác suất dự đoán prob.\n",
        "        prob = self.sigmoid(X @ self.theta)\n",
        "        # TODO: 2. Áp dụng ngưỡng 0.5 để đưa ra nhãn cuối cùng (0 hoặc 1).\n",
        "        return (prob >= 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHsQmOGAzLQl",
        "outputId": "cf79450d-a6cf-4697-bda5-946c754e26b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter    1: Loss = 0.411776\n",
            "Iter    2: Loss = 0.343864\n",
            "Iter    3: Loss = 0.327153\n",
            "Iter    4: Loss = 0.325591\n",
            "Iter    5: Loss = 0.325572\n",
            "Iter    6: Loss = 0.325572\n",
            "Converged after 7 iterations. Final Loss = 0.325572\n",
            "Final Theta: [-6.26018491  2.47707251 -0.0299125 ]\n"
          ]
        }
      ],
      "source": [
        "# Uncomment\n",
        "\n",
        "model = LogisticNewtonMethod(max_iter=100, verbose=True)\n",
        "model.fit(x_train, y_train)\n",
        "learned_theta_newton = model.theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hxhcEQuw1O6",
        "outputId": "4a7bc80a-cbe4-495c-c95a-954b17f7676f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Accuracy on Training Data: 0.88\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(x_valid)\n",
        "accuracy = np.mean(y_pred == y_valid)\n",
        "print(f\"\\nFinal Accuracy on Valid Data: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91f72d5b",
        "outputId": "5d55588f-c5a2-49ec-ae1a-55fcc5f72be5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on Training Data (sklearn Logistic Regression): 0.88\n",
            "\n",
            "Learned Parameters (sklearn):\n",
            "Coefficients: [-6.26018989  2.47707379 -0.0299125 ]\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression(penalty=None, fit_intercept = False)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_train_pred_sklearn = model.predict(x_valid)\n",
        "accuracy_train_sklearn = accuracy_score(y_valid, y_train_pred_sklearn)\n",
        "\n",
        "print(f\"Accuracy on Valid Data: {accuracy_train_sklearn:.2f}\")\n",
        "print(\"\\nLearned Parameters (sklearn):\")\n",
        "print(\"Coefficients:\", model.coef_.flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq5BBqA4OweN"
      },
      "source": [
        "# **Bài tập 3:** Sử dụng hai phương pháp kể trên với bộ dữ liệu Titanic dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAvwVOWlJp1s"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/titanic.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnvHYhb7yMBE"
      },
      "outputs": [],
      "source": [
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "target = \"Survived\"\n",
        "\n",
        "df = df[features + [target]].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smfmofbtyN79"
      },
      "outputs": [],
      "source": [
        "df[\"Sex\"] = df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
        "df[\"Embarked\"] = df[\"Embarked\"].map({\"S\": 0, \"C\": 1, \"Q\": 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wgha0bbyPo2"
      },
      "outputs": [],
      "source": [
        "X = df[features].values\n",
        "y = df[target].values.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPFKaSEPyQ47"
      },
      "outputs": [],
      "source": [
        "y = df[target].values.astype(int)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HptgTkdybGH",
        "outputId": "60c2aacd-ddb3-4dc8-ed72-e4b5f643d116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: (891, 7)\n",
            "y shape: (891,)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBGWWEXwyuyT"
      },
      "outputs": [],
      "source": [
        "median_age_train = np.nanmedian(X_train[:, features.index(\"Age\")])\n",
        "mode_embarked_train = np.nanmax(X_train[:, features.index(\"Embarked\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvEfk8Rtz9O5"
      },
      "outputs": [],
      "source": [
        "age_na_index = np.isnan(X_train[:, features.index(\"Age\")])\n",
        "embarked_na_index = np.isnan(X_train[:, features.index(\"Embarked\")])\n",
        "X_train[:, features.index(\"Age\")][age_na_index] = median_age_train\n",
        "X_train[:, features.index(\"Embarked\")][embarked_na_index] = mode_embarked_train\n",
        "\n",
        "age_na_index = np.isnan(X_test[:, features.index(\"Age\")])\n",
        "embarked_na_index = np.isnan(X_test[:, features.index(\"Embarked\")])\n",
        "X_test[:, features.index(\"Age\")][age_na_index] = median_age_train\n",
        "X_test[:, features.index(\"Embarked\")][embarked_na_index] = mode_embarked_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-FtDiRI3gWB"
      },
      "outputs": [],
      "source": [
        "X_train_mean = X_train.mean(axis=0)\n",
        "X_train_std = X_train.std(axis=0)\n",
        "\n",
        "X_train = (X_train - X_train_mean) / X_train_std\n",
        "X_test = (X_test - X_train_mean) / X_train_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MAPtGPz3h4_"
      },
      "outputs": [],
      "source": [
        "X_train = add_intercept(X_train)\n",
        "X_test  = add_intercept(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXbWfJXCMd_M"
      },
      "source": [
        "## Huấn luyện với hai mô hình tự implement ở trên và sklearn\n",
        "- In ra accuracy của các mô hình trên tập test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDoRSnsmydG4"
      },
      "outputs": [],
      "source": [
        "# SimpleLogisticRegression\n",
        "model = SimpleLogisticRegression(alpha=0.1, max_iter=1000, verbose=True)\n",
        "model.fit(X_train, y_train)\n",
        "learned_theta_titanic = model.theta\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f\"Accuracy on Titanic Test Data: {accuracy:.2f}\")\n",
        "print(\"\\nLearned Parameters (theta) on Titanic Dataset:\")\n",
        "print(learned_theta_titanic) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI73MJBpyfx4"
      },
      "outputs": [],
      "source": [
        "# LogisticNewtonMethod\n",
        "model = LogisticNewtonMethod(max_iter=100, verbose=True)\n",
        "model.fit(X_train, y_train)\n",
        "learned_theta_titanic_newton = model.theta\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f\"Accuracy on Titanic Test Data (Newton's Method): {accuracy:.2f}\")\n",
        "print(\"\\nLearned Parameters (theta) on Titanic Dataset (Newton's Method):\")\n",
        "print(learned_theta_titanic_newton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml6fIMvbJPBL"
      },
      "outputs": [],
      "source": [
        "# sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(penalty=None, fit_intercept = False)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "test_acc  = (y_pred_test  == y_test ).mean()\n",
        "\n",
        "print(model.coef_)\n",
        "print(f\"Test Accuracy:  {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hf_fccKqok1"
      },
      "source": [
        "#V. Softmax Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTtyIszI9MKV"
      },
      "source": [
        "Qua hai ví dụ trước, ta thấy Logistic Regression trong bài toán phân loại sử dụng Gradient Descent và Newton với hàm sigmoid bị giới hạn trong phân loại nhị phân với hai nhãn $y \\in \\{0,1\\}$. Vậy nếu chúng ta muốn giải quyết các bài toán nơi có nhiều hơn 2 nhãn thì phải làm như thế nào ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khzfQz3l43SF"
      },
      "source": [
        "## 1. Hồi quy Softmax (Multinomial Logistic Regression)\n",
        "\n",
        "Để mở rộng Logistic Regression từ phân loại nhị phân sang phân loại đa lớp ($K > 2$ nhãn), ta sử dụng **Hồi quy Softmax**. Phương pháp này thay thế hàm sigmoid bằng hàm Softmax và sử dụng một tập hợp các vector tham số $\\mathbf{\\theta}_1, \\dots, \\mathbf{\\theta}_K$, mỗi vector tương ứng với một lớp.\n",
        "\n",
        "### a. Hàm Softmax (Softmax Function)\n",
        "\n",
        "Hàm Softmax tính toán xác suất $\\hat{p}_k$ một mẫu $\\mathbf{x}$ thuộc lớp $k$ trong tổng số $K$ lớp:\n",
        "\n",
        "$$P(y=k \\mid \\mathbf{x} ; \\mathbf{\\Theta}) = \\hat{p}_k = \\frac{e^{\\mathbf{\\theta}_k^T \\mathbf{x}}}{\\sum_{j=1}^{K} e^{\\mathbf{\\theta}_j^T \\mathbf{x}}}$$\n",
        "\n",
        "Trong đó, $\\mathbf{\\Theta} = \\{\\mathbf{\\theta}_1, \\dots, \\mathbf{\\theta}_K\\}$ là tập hợp tất cả các tham số của mô hình.\n",
        "\n",
        "### b. Hàm Mất mát (Cost Function)\n",
        "\n",
        "Hàm mất mát tiêu chuẩn là **Cross-Entropy Loss**:\n",
        "\n",
        "$$J(\\mathbf{\\Theta}) = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\mathbf{1}\\{y^{(i)} = k\\} \\log(\\hat{p}_k^{(i)})$$\n",
        "\n",
        "$\\mathbf{1}\\{y^{(i)} = k\\}$ là hàm chỉ thị, bằng $1$ nếu mẫu $i$ thuộc lớp $k$.\n",
        "\n",
        "### c. Tối ưu hóa bằng Gradient Descent\n",
        "\n",
        "Các tham số $\\mathbf{\\Theta}$ được tối ưu hóa thông qua **Gradient Descent** bằng cách sử dụng đạo hàm riêng sau:\n",
        "\n",
        "$$\\nabla_{\\mathbf{\\theta}_k} J(\\mathbf{\\Theta}) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\hat{p}_k^{(i)} - \\mathbf{1}\\{y^{(i)} = k\\} \\right) \\mathbf{x}^{(i)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDNJWUlKkMKI"
      },
      "source": [
        "### a. Load dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDy2rsCoif6n",
        "outputId": "cb30eed5-4810-466f-b196-838c21d9a9b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  iris.zip\n",
            "replace Index? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!unzip iris.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDfE-_NriiQb"
      },
      "outputs": [],
      "source": [
        "DATA_FILE = 'iris.data'\n",
        "\n",
        "column_names = [\n",
        "    'sepal_length',\n",
        "    'sepal_width',\n",
        "    'petal_length',\n",
        "    'petal_width',\n",
        "    'species'\n",
        "]\n",
        "\n",
        "df = pd.read_csv(DATA_FILE, header=None, names=column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVsStPhlkpiA"
      },
      "source": [
        "### b.Encode Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wldT_ZBaiqA0",
        "outputId": "feef70f6-1687-4907-fa32-ab6287daa034"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Iris-setosa': np.int64(0), 'Iris-versicolor': np.int64(1), 'Iris-virginica': np.int64(2)}\n"
          ]
        }
      ],
      "source": [
        "le = LabelEncoder()\n",
        "df['species_encoded'] = le.fit_transform(df['species'])\n",
        "\n",
        "encoding_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(encoding_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjC0PLDgktTf"
      },
      "source": [
        "### c. Chia dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMQ73k9IjBjJ"
      },
      "outputs": [],
      "source": [
        "X = df.iloc[:, :-2]\n",
        "y = df['species_encoded']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydG3LD1li9R-",
        "outputId": "418b8f4c-f810-4383-d233-20161412c08a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Data Split Summary ---\n",
            "X_train shape: (105, 4)\n",
            "X_test shape: (45, 4)\n",
            "y_train shape: (105,)\n",
            "y_test shape: (45,)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Data Split Summary ---\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR_wiaYC4NfK"
      },
      "outputs": [],
      "source": [
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "y_train_ohe = ohe.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "X_train_b = add_intercept(X_train.values)\n",
        "X_test_b = add_intercept(X_test.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGcsE_FkkwpO"
      },
      "source": [
        "### d. Softmax Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSCvWvOESsKR"
      },
      "source": [
        "# **Bài tập 4:** Hoàn thiện Class `SoftmaxRegression`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMLUlcS77oV3"
      },
      "outputs": [],
      "source": [
        "class SoftmaxRegression(object):\n",
        "    def __init__(self, alpha=0.01, eps=1e-6, max_iter=1000, verbose: bool = False):\n",
        "        self.theta = None\n",
        "        self.alpha = alpha\n",
        "        self.eps = eps\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "        self.classes = 0\n",
        "\n",
        "    def softmax(self, z):\n",
        "        # TODO: 1. Tính lũy thừa e^z.\n",
        "        exp_z = np.exp(z)\n",
        "\n",
        "        # TODO: 2. Tính tổng lũy thừa (mẫu số) để chuẩn hóa.\n",
        "        sum_exp_z = np.sum(exp_z, axis=1, keepdims=True)\n",
        "        # Lưu ý axis=1: Tính tổng các lũy thừa trên TỪNG HÀNG (tức là tổng các lớp) cho TỪNG mẫu.\n",
        "        # Lưu ý keepdims=True: Giữ kích thước (mẫu, 1) để có thể chia (Broadcast)\n",
        "        # chính xác với exp_z (kích thước mẫu, lớp).\n",
        "\n",
        "\n",
        "        # TODO: 3. Trả về kết quả Softmax (exp_z / sum_exp_z).\n",
        "        return exp_z / sum_exp_z\n",
        "\n",
        "    def cross_entropy_loss(self, X, Y):\n",
        "        m = X.shape[0]\n",
        "\n",
        "        logits = X @ self.theta\n",
        "        # TODO: 1. Tính logits = tích X và theta.\n",
        "        logits = X @ self.theta\n",
        "        # TODO: 2. Tính softmax(logits).\n",
        "        P = self.softmax(logits)\n",
        "        # TODO: 3. Tính loss rồi return trả về loss.\n",
        "        loss = - (1 / m) * np.sum(Y * np.log(P + self.eps))\n",
        "        return loss\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        m, n_features = X.shape\n",
        "        # TODO: 1. Xác định số lớp (self.classes) từ Y.\n",
        "        self.classes = Y.shape[1]\n",
        "        # TODO: 2. Khởi tạo self.theta bằng np.zeros với kích thước (n_features, self.classes).\n",
        "        self.theta = np.zeros((n_features, self.classes))\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "\n",
        "            # TODO: 3. Tính logits, sau đó tính xác suất.\n",
        "            logits = X @ self.theta\n",
        "            P = self.softmax(logits)\n",
        "\n",
        "            # TODO: 4. Tính sai số\n",
        "            error = P - Y\n",
        "\n",
        "            # TODO: 5. Tính Gradient:\n",
        "            grad = (1 / m) * (X.T @ error)\n",
        "\n",
        "            # TODO: 6. Cập nhật tham số: new_theta\n",
        "            new_theta = self.theta - self.alpha * grad\n",
        "\n",
        "            # TODO: 7. KIỂM TRA HỘI TỤ (Convergence Check):\n",
        "            # Dùng np.linalg.norm(new_theta - self.theta, 1) < self.eps.\n",
        "            # Nếu hội tụ, cập nhật self.theta và kết thúc (return).\n",
        "            if np.linalg.norm(new_theta - self.theta, 1) < self.eps:\n",
        "                self.theta = new_theta\n",
        "                return\n",
        "            # TODO: 8. Cập nhật self.theta = new_theta.\n",
        "            self.theta = new_theta\n",
        "\n",
        "            # TODO: 9. (Tùy chọn) In thông tin mất mát nếu self.verbose là True.\n",
        "            if self.verbose and i % 100 == 0:\n",
        "                print(f\"Iteration {i}: Loss = {loss:.4f}\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # TODO: 1. Tính logits\n",
        "        logits = X @ self.theta\n",
        "\n",
        "        # TODO: 2. Trả về xác suất P bằng cách gọi self.softmax(logits).\n",
        "        return self.softmax(logits)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # TODO: 1. Tính Ma trận xác suất P bằng cách gọi self.predict_proba(X).\n",
        "        P = self.predict_proba(X)\n",
        "        return np.argmax(P, axis=1)\n",
        "        # TODO: 2. Tìm chỉ mục (index) của giá trị lớn nhất trên mỗi hàng (axis=1).\n",
        "        return np.argmax(P, axis=1)\n",
        "\n",
        "        # Dùng np.argmax sẽ trả về nhãn lớp có xác suất cao nhất."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4gNJeIGk6Gg"
      },
      "source": [
        "### e. Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5tLdiclXe7l",
        "outputId": "a80e7378-c3d8-4658-f02f-b80852534aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter  100: Loss = 1.019684\n",
            "Iter  200: Loss = 0.964624\n",
            "Iter  300: Loss = 0.916659\n",
            "Iter  400: Loss = 0.874650\n",
            "Iter  500: Loss = 0.837781\n",
            "Iter  600: Loss = 0.805317\n",
            "Iter  700: Loss = 0.776615\n",
            "Iter  800: Loss = 0.751119\n",
            "Iter  900: Loss = 0.728362\n",
            "Iter 1000: Loss = 0.707947\n",
            "Iter 1100: Loss = 0.689541\n",
            "Iter 1200: Loss = 0.672865\n",
            "Iter 1300: Loss = 0.657686\n",
            "Iter 1400: Loss = 0.643807\n",
            "Iter 1500: Loss = 0.631062\n",
            "Iter 1600: Loss = 0.619310\n",
            "Iter 1700: Loss = 0.608432\n",
            "Iter 1800: Loss = 0.598326\n",
            "Iter 1900: Loss = 0.588906\n",
            "Iter 2000: Loss = 0.580097\n",
            "Iter 2100: Loss = 0.571834\n",
            "Iter 2200: Loss = 0.564061\n",
            "Iter 2300: Loss = 0.556729\n",
            "Iter 2400: Loss = 0.549797\n",
            "Iter 2500: Loss = 0.543227\n",
            "Iter 2600: Loss = 0.536985\n",
            "Iter 2700: Loss = 0.531044\n",
            "Iter 2800: Loss = 0.525378\n",
            "Iter 2900: Loss = 0.519963\n",
            "Iter 3000: Loss = 0.514781\n",
            "Iter 3100: Loss = 0.509812\n",
            "Iter 3200: Loss = 0.505040\n",
            "Iter 3300: Loss = 0.500451\n",
            "Iter 3400: Loss = 0.496032\n",
            "Iter 3500: Loss = 0.491771\n",
            "Iter 3600: Loss = 0.487657\n",
            "Iter 3700: Loss = 0.483680\n",
            "Iter 3800: Loss = 0.479832\n",
            "Iter 3900: Loss = 0.476105\n",
            "Iter 4000: Loss = 0.472491\n",
            "Iter 4100: Loss = 0.468982\n",
            "Iter 4200: Loss = 0.465575\n",
            "Iter 4300: Loss = 0.462261\n",
            "Iter 4400: Loss = 0.459037\n",
            "Iter 4500: Loss = 0.455898\n",
            "Iter 4600: Loss = 0.452838\n",
            "Iter 4700: Loss = 0.449854\n",
            "Iter 4800: Loss = 0.446943\n",
            "Iter 4900: Loss = 0.444099\n",
            "Iter 5000: Loss = 0.441322\n",
            "Iter 5100: Loss = 0.438606\n",
            "Iter 5200: Loss = 0.435949\n",
            "Iter 5300: Loss = 0.433350\n",
            "Iter 5400: Loss = 0.430804\n",
            "Iter 5500: Loss = 0.428310\n",
            "Iter 5600: Loss = 0.425866\n",
            "Iter 5700: Loss = 0.423470\n",
            "Iter 5800: Loss = 0.421119\n",
            "Iter 5900: Loss = 0.418812\n",
            "Iter 6000: Loss = 0.416548\n",
            "Iter 6100: Loss = 0.414324\n",
            "Iter 6200: Loss = 0.412139\n",
            "Iter 6300: Loss = 0.409992\n",
            "Iter 6400: Loss = 0.407881\n",
            "Iter 6500: Loss = 0.405806\n",
            "Iter 6600: Loss = 0.403764\n",
            "Iter 6700: Loss = 0.401755\n",
            "Iter 6800: Loss = 0.399778\n",
            "Iter 6900: Loss = 0.397831\n",
            "Iter 7000: Loss = 0.395914\n",
            "Iter 7100: Loss = 0.394026\n",
            "Iter 7200: Loss = 0.392166\n",
            "Iter 7300: Loss = 0.390333\n",
            "Iter 7400: Loss = 0.388526\n",
            "Iter 7500: Loss = 0.386744\n",
            "Iter 7600: Loss = 0.384988\n",
            "Iter 7700: Loss = 0.383255\n",
            "Iter 7800: Loss = 0.381546\n",
            "Iter 7900: Loss = 0.379860\n",
            "Iter 8000: Loss = 0.378195\n",
            "Iter 8100: Loss = 0.376553\n",
            "Iter 8200: Loss = 0.374931\n",
            "Iter 8300: Loss = 0.373330\n",
            "Iter 8400: Loss = 0.371749\n",
            "Iter 8500: Loss = 0.370188\n",
            "Iter 8600: Loss = 0.368646\n",
            "Iter 8700: Loss = 0.367122\n",
            "Iter 8800: Loss = 0.365616\n",
            "Iter 8900: Loss = 0.364128\n",
            "Iter 9000: Loss = 0.362658\n",
            "Iter 9100: Loss = 0.361204\n",
            "Iter 9200: Loss = 0.359767\n",
            "Iter 9300: Loss = 0.358346\n",
            "Iter 9400: Loss = 0.356942\n",
            "Iter 9500: Loss = 0.355552\n",
            "Iter 9600: Loss = 0.354178\n",
            "Iter 9700: Loss = 0.352819\n",
            "Iter 9800: Loss = 0.351474\n",
            "Iter 9900: Loss = 0.350144\n",
            "Iter 10000: Loss = 0.348828\n",
            "Finished training (max iterations reached). Final Loss = 0.348828\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Trained Parameters [[ 0.17363455  0.09946417 -0.27309872]\n",
            " [ 0.31211908  0.20845666 -0.52057575]\n",
            " [ 0.84127628 -0.32226583 -0.51901045]\n",
            " [-1.18874289  0.17527213  1.01347076]\n",
            " [-0.56146676 -0.18906691  0.75053366]] and Shape (5, 3)\n",
            "Accuracy on Test Set: 95.56%\n"
          ]
        }
      ],
      "source": [
        "model = SoftmaxRegression(alpha=0.001, max_iter=10000, verbose=True, eps = 1e-6)\n",
        "model.fit(X_train_b, y_train_ohe)\n",
        "\n",
        "theta_trained = model.theta\n",
        "\n",
        "y_test_pred = model.predict(X_test_b)\n",
        "\n",
        "test_accuracy = np.mean(y_test == y_test_pred)\n",
        "\n",
        "print(f\"\\n--- Model Evaluation ---\")\n",
        "print(f\"Trained Parameters {theta_trained} and Shape {theta_trained.shape}\")\n",
        "print(f\"Accuracy on Test Set: {test_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDkpGpUj6ARD"
      },
      "outputs": [],
      "source": [
        "conf_mat = confusion_matrix(y_test, y_test_pred)\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(conf_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_test_pred)\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(class_report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
